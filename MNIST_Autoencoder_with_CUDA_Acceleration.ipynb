{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "id": "Kxg4hS5iWEq3",
        "outputId": "4a32d414-7fbd-4c03-cbd1-c098ba10066e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.14-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.2.2)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
            "Downloading pytools-2024.1.14-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp310-cp310-linux_x86_64.whl size=660544 sha256=fc456349a5ed5e1b9262770bdd9dab7c02f33163ed1e1620e7ef4c068a2a0d95\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/63/40/4bf006182f942d3516b71bb2ff3b57ccbdb8b2c0ee81882b6e\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.5 pycuda-2024.1.2 pytools-2024.1.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "images = mnist.data.astype(np.float32) / 255.0\n",
        "images = np.ascontiguousarray(images)\n",
        "\n",
        "# CUDA kernel for forward and backward pass with ReLU and batch normalization\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void forward_pass(float *input, float *weights, float *output, float *bn_mean, float *bn_var, float *bn_gamma, float *bn_beta, int input_size, int output_size)\n",
        "{\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < output_size) {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            sum += input[i] * weights[idx * input_size + i];\n",
        "        }\n",
        "        // Batch normalization\n",
        "        sum = (sum - bn_mean[idx]) / sqrt(bn_var[idx] + 1e-5);\n",
        "        sum = bn_gamma[idx] * sum + bn_beta[idx];\n",
        "        // ReLU activation\n",
        "        output[idx] = fmaxf(sum, 0);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backward_pass(float *input, float *weights, float *output, float *output_grad, float *input_grad, float *weight_grad, float *bn_gamma_grad, float *bn_beta_grad, float *bn_mean, float *bn_var, float *bn_gamma, float *bn_beta, int input_size, int output_size)\n",
        "{\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < input_size) {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < output_size; i++) {\n",
        "            float bn_grad = output_grad[i] * (output[i] > 0 ? 1 : 0);  // ReLU gradient\n",
        "            bn_grad *= bn_gamma[i] / sqrt(bn_var[i] + 1e-5);\n",
        "            sum += bn_grad * weights[i * input_size + idx];\n",
        "            atomicAdd(&weight_grad[i * input_size + idx], bn_grad * input[idx]);\n",
        "            atomicAdd(&bn_gamma_grad[i], output_grad[i] * (output[i] - bn_beta[i]) / bn_gamma[i]);\n",
        "            atomicAdd(&bn_beta_grad[i], output_grad[i]);\n",
        "        }\n",
        "        input_grad[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "forward_pass = mod.get_function(\"forward_pass\")\n",
        "backward_pass = mod.get_function(\"backward_pass\")\n",
        "\n",
        "def run_forward(input_data, weights, output_size, bn_mean, bn_var, bn_gamma, bn_beta):\n",
        "    output = np.zeros(output_size, dtype=np.float32)\n",
        "    forward_pass(\n",
        "        cuda.In(input_data), cuda.In(weights), cuda.Out(output),\n",
        "        cuda.In(bn_mean), cuda.In(bn_var), cuda.In(bn_gamma), cuda.In(bn_beta),\n",
        "        np.int32(input_data.shape[0]), np.int32(output_size),\n",
        "        block=(256, 1, 1), grid=((output_size + 255) // 256, 1)\n",
        "    )\n",
        "    return output\n",
        "\n",
        "def run_backward(input_data, weights, output, output_grad, input_size, output_size, bn_mean, bn_var, bn_gamma, bn_beta):\n",
        "    input_grad = np.zeros(input_size, dtype=np.float32)\n",
        "    weight_grad = np.zeros((output_size, input_size), dtype=np.float32)\n",
        "    bn_gamma_grad = np.zeros(output_size, dtype=np.float32)\n",
        "    bn_beta_grad = np.zeros(output_size, dtype=np.float32)\n",
        "    backward_pass(\n",
        "        cuda.In(input_data), cuda.In(weights), cuda.In(output), cuda.In(output_grad),\n",
        "        cuda.Out(input_grad), cuda.Out(weight_grad), cuda.Out(bn_gamma_grad), cuda.Out(bn_beta_grad),\n",
        "        cuda.In(bn_mean), cuda.In(bn_var), cuda.In(bn_gamma), cuda.In(bn_beta),\n",
        "        np.int32(input_size), np.int32(output_size),\n",
        "        block=(256, 1, 1), grid=((input_size + 255) // 256, 1)\n",
        "    )\n",
        "    return input_grad, weight_grad, bn_gamma_grad, bn_beta_grad\n",
        "\n",
        "class Autoencoder:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.weights1 = np.random.randn(hidden_size, input_size).astype(np.float32) * np.sqrt(2.0 / input_size)\n",
        "        self.weights2 = np.random.randn(input_size, hidden_size).astype(np.float32) * np.sqrt(2.0 / hidden_size)\n",
        "        self.bn1_mean = np.zeros(hidden_size, dtype=np.float32)\n",
        "        self.bn1_var = np.ones(hidden_size, dtype=np.float32)\n",
        "        self.bn1_gamma = np.ones(hidden_size, dtype=np.float32)\n",
        "        self.bn1_beta = np.zeros(hidden_size, dtype=np.float32)\n",
        "        self.bn2_mean = np.zeros(input_size, dtype=np.float32)\n",
        "        self.bn2_var = np.ones(input_size, dtype=np.float32)\n",
        "        self.bn2_gamma = np.ones(input_size, dtype=np.float32)\n",
        "        self.bn2_beta = np.zeros(input_size, dtype=np.float32)\n",
        "\n",
        "        # Adam optimizer parameters\n",
        "        self.m1, self.v1 = np.zeros_like(self.weights1), np.zeros_like(self.weights1)\n",
        "        self.m2, self.v2 = np.zeros_like(self.weights2), np.zeros_like(self.weights2)\n",
        "        self.t = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.hidden = run_forward(x, self.weights1, self.hidden_size, self.bn1_mean, self.bn1_var, self.bn1_gamma, self.bn1_beta)\n",
        "        self.output = run_forward(self.hidden, self.weights2, self.input_size, self.bn2_mean, self.bn2_var, self.bn2_gamma, self.bn2_beta)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, x, output_grad):\n",
        "        hidden_grad, weight2_grad, bn2_gamma_grad, bn2_beta_grad = run_backward(self.hidden, self.weights2, self.output, output_grad, self.hidden_size, self.input_size, self.bn2_mean, self.bn2_var, self.bn2_gamma, self.bn2_beta)\n",
        "        _, weight1_grad, bn1_gamma_grad, bn1_beta_grad = run_backward(x, self.weights1, self.hidden, hidden_grad, self.input_size, self.hidden_size, self.bn1_mean, self.bn1_var, self.bn1_gamma, self.bn1_beta)\n",
        "        return weight1_grad, weight2_grad, bn1_gamma_grad, bn1_beta_grad, bn2_gamma_grad, bn2_beta_grad\n",
        "\n",
        "    def adam_update(self, grad, m, v, t, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "        m_hat = m / (1 - beta1 ** t)\n",
        "        v_hat = v / (1 - beta2 ** t)\n",
        "        update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "        return update, m, v\n",
        "\n",
        "    def train(self, x, learning_rate, l2_reg=1e-5):\n",
        "        self.t += 1\n",
        "        output = self.forward(x)\n",
        "        error = output - x\n",
        "        weight1_grad, weight2_grad, bn1_gamma_grad, bn1_beta_grad, bn2_gamma_grad, bn2_beta_grad = self.backward(x, error)\n",
        "\n",
        "        # L2 regularization\n",
        "        weight1_grad += l2_reg * self.weights1\n",
        "        weight2_grad += l2_reg * self.weights2\n",
        "\n",
        "        # Adam updates\n",
        "        w1_update, self.m1, self.v1 = self.adam_update(weight1_grad, self.m1, self.v1, self.t, learning_rate)\n",
        "        w2_update, self.m2, self.v2 = self.adam_update(weight2_grad, self.m2, self.v2, self.t, learning_rate)\n",
        "\n",
        "        self.weights1 -= w1_update\n",
        "        self.weights2 -= w2_update\n",
        "        self.bn1_gamma -= learning_rate * bn1_gamma_grad\n",
        "        self.bn1_beta -= learning_rate * bn1_beta_grad\n",
        "        self.bn2_gamma -= learning_rate * bn2_gamma_grad\n",
        "        self.bn2_beta -= learning_rate * bn2_beta_grad\n",
        "\n",
        "        return np.mean(error**2)\n",
        "\n",
        "# Training loop\n",
        "autoencoder = Autoencoder(784, 256)\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "initial_learning_rate = 0.0001\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    # Cosine annealing learning rate schedule\n",
        "    learning_rate = initial_learning_rate * (1 + np.cos(epoch * np.pi / epochs)) / 2\n",
        "\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = images[i:i+batch_size]\n",
        "        batch = np.ascontiguousarray(batch)\n",
        "        loss = autoencoder.train(batch, learning_rate)\n",
        "        total_loss += loss\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/(len(images)//batch_size):.6f}, LR: {learning_rate:.6f}\")\n",
        "\n",
        "# Visualize results\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(images[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    input_image = np.ascontiguousarray(images[i])\n",
        "    plt.imshow(autoencoder.forward(input_image).reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Da6Pe65_V4KQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82fedead-5265-429a-811f-e5e74998e788"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_comm.py:658: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  frame = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.078407, LR: 0.000100\n",
            "Epoch 2/50, Loss: 0.074671, LR: 0.000100\n",
            "Epoch 3/50, Loss: 0.071510, LR: 0.000100\n",
            "Epoch 4/50, Loss: 0.071601, LR: 0.000099\n",
            "Epoch 5/50, Loss: 0.075897, LR: 0.000098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-80b4488ea110>:108: RuntimeWarning: overflow encountered in square\n",
            "  v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
            "<ipython-input-2-80b4488ea110>:111: RuntimeWarning: invalid value encountered in divide\n",
            "  update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
            "<ipython-input-2-80b4488ea110>:135: RuntimeWarning: overflow encountered in square\n",
            "  return np.mean(error**2)\n",
            "<ipython-input-2-80b4488ea110>:132: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.bn2_gamma -= learning_rate * bn2_gamma_grad\n",
            "<ipython-input-2-80b4488ea110>:133: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.bn2_beta -= learning_rate * bn2_beta_grad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Loss: inf, LR: 0.000098\n",
            "Epoch 7/50, Loss: inf, LR: 0.000096\n",
            "Epoch 8/50, Loss: inf, LR: 0.000095\n",
            "Epoch 9/50, Loss: inf, LR: 0.000094\n",
            "Epoch 10/50, Loss: inf, LR: 0.000092\n",
            "Epoch 11/50, Loss: inf, LR: 0.000090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Loss: inf, LR: 0.000089\n",
            "Epoch 13/50, Loss: inf, LR: 0.000086\n",
            "Epoch 14/50, Loss: 0.112486, LR: 0.000084\n",
            "Epoch 15/50, Loss: 0.112486, LR: 0.000082\n",
            "Epoch 16/50, Loss: 0.112486, LR: 0.000079\n",
            "Epoch 17/50, Loss: 0.112486, LR: 0.000077\n",
            "Epoch 18/50, Loss: 0.112486, LR: 0.000074\n",
            "Epoch 19/50, Loss: 0.112486, LR: 0.000071\n",
            "Epoch 20/50, Loss: 0.112486, LR: 0.000068\n",
            "Epoch 21/50, Loss: 0.112486, LR: 0.000065\n",
            "Epoch 22/50, Loss: 0.112486, LR: 0.000062\n",
            "Epoch 23/50, Loss: 0.112486, LR: 0.000059\n",
            "Epoch 24/50, Loss: 0.112486, LR: 0.000056\n",
            "Epoch 25/50, Loss: 0.112486, LR: 0.000053\n",
            "Epoch 26/50, Loss: 0.112486, LR: 0.000050\n",
            "Epoch 27/50, Loss: 0.112486, LR: 0.000047\n",
            "Epoch 28/50, Loss: 0.112486, LR: 0.000044\n",
            "Epoch 29/50, Loss: 0.112486, LR: 0.000041\n",
            "Epoch 30/50, Loss: 0.112486, LR: 0.000038\n",
            "Epoch 31/50, Loss: 0.112486, LR: 0.000035\n",
            "Epoch 32/50, Loss: 0.112486, LR: 0.000032\n",
            "Epoch 33/50, Loss: 0.112486, LR: 0.000029\n",
            "Epoch 34/50, Loss: 0.112486, LR: 0.000026\n",
            "Epoch 35/50, Loss: 0.112486, LR: 0.000023\n",
            "Epoch 36/50, Loss: 0.112486, LR: 0.000021\n",
            "Epoch 37/50, Loss: 0.112486, LR: 0.000018\n",
            "Epoch 38/50, Loss: 0.112486, LR: 0.000016\n",
            "Epoch 39/50, Loss: 0.112486, LR: 0.000014\n",
            "Epoch 40/50, Loss: 0.112486, LR: 0.000011\n",
            "Epoch 41/50, Loss: 0.112486, LR: 0.000010\n",
            "Epoch 42/50, Loss: 0.112486, LR: 0.000008\n",
            "Epoch 43/50, Loss: 0.112486, LR: 0.000006\n",
            "Epoch 44/50, Loss: 0.112486, LR: 0.000005\n",
            "Epoch 45/50, Loss: 0.112486, LR: 0.000004\n",
            "Epoch 46/50, Loss: 0.112486, LR: 0.000002\n",
            "Epoch 47/50, Loss: 0.112486, LR: 0.000002\n",
            "Epoch 48/50, Loss: 0.112486, LR: 0.000001\n",
            "Epoch 49/50, Loss: 0.112486, LR: 0.000000\n",
            "Epoch 50/50, Loss: 0.112486, LR: 0.000000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x400 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}